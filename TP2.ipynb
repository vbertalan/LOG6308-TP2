{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47aa9bf",
   "metadata": {},
   "source": [
    "### TP2\n",
    "\n",
    "\n",
    "### Mohammed Ramzi Bouthiba, Matricule 2065386\n",
    "### Vithor Bertalan, Matricule 2135362"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cc0b5",
   "metadata": {},
   "source": [
    "### First Step - Load the CSVs, and calculate the adjacencies graph using iGraph. Put the CSVs in the same folder as this Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a4c69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import time\n",
    "\n",
    "## Loads matrix and CSVs\n",
    "m = scipy.io.mmread('tp-matrix.dgt')\n",
    "articles = pd.read_csv(\"12-articles.csv\", sep=\",\")\n",
    "matrix_names = pd.read_csv(\"tp-matrix-names.csv\", sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d928e15",
   "metadata": {},
   "source": [
    "### Q1, Part 1 - Calculate 10 recommendations of each of the articles in the 12-articles subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0639fd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For paper number 15089, number 1 in the 12-articles set, the recommendations by PageRank are:\n",
      "[6568, 6106, 3266, 3269, 9137, 3263, 3268, 3277, 13231, 16362, 8132, 22252, 18172]\n",
      "For paper number 35353, number 2 in the 12-articles set, the recommendations by PageRank are:\n",
      "[2004, 2002, 308, 3187, 6994, 333, 7892, 338, 5907, 282, 10632, 1059, 12769, 12467, 25724, 12764, 24730, 1348]\n",
      "For paper number 50496, number 3 in the 12-articles set, the recommendations by PageRank are:\n",
      "[13710, 30836, 21335]\n",
      "For paper number 50497, number 4 in the 12-articles set, the recommendations by PageRank are:\n",
      "[97, 281, 107]\n",
      "For paper number 11636, number 5 in the 12-articles set, the recommendations by PageRank are:\n",
      "[33, 1642, 1085, 1250, 2271, 2083, 1323, 1368, 4941, 2079, 3192, 4024, 2859, 4159, 6204, 6095]\n",
      "For paper number 12593, number 6 in the 12-articles set, the recommendations by PageRank are:\n",
      "[17, 449, 16, 1562, 2040, 6464, 373, 3228, 3503, 4259, 5559, 1874, 4705, 15802, 9063, 5197, 9066, 3043, 3221, 3035, 3145, 3140, 9665, 9056, 3138, 9527, 28864, 6522, 9060, 8312, 24434, 24533, 8662, 372, 8657, 31948]\n",
      "For paper number 36565, number 7 in the 12-articles set, the recommendations by PageRank are:\n",
      "[76, 811, 939, 696, 334, 703, 483, 8355, 5103, 7341, 2983, 3411, 7413, 6321, 8044]\n",
      "For paper number 12215, number 8 in the 12-articles set, the recommendations by PageRank are:\n",
      "[1391, 2210, 1043, 6716, 4975, 7113, 11776, 1039, 465, 1037, 6714, 12217, 12218, 12216, 9523, 3583]\n",
      "For paper number 18645, number 9 in the 12-articles set, the recommendations by PageRank are:\n",
      "[159, 3773, 611, 2765, 160, 4965, 876, 9797, 3770, 3440, 8563, 4064, 1034, 6731, 468, 8168, 10844, 4092, 6945, 18724, 9126, 293, 69, 278, 18656, 3348, 2257, 20156, 11641, 4180, 23816, 6974, 8927, 1653, 1755, 7307, 18885, 37995]\n",
      "For paper number 1594, number 10 in the 12-articles set, the recommendations by PageRank are:\n",
      "[2792, 8425, 8021, 1006, 9315, 9310, 9319, 13054, 18760, 11993, 9311, 9308, 13384, 19336]\n",
      "For paper number 35304, number 11 in the 12-articles set, the recommendations by PageRank are:\n",
      "[864, 3887, 4799, 15177, 15176, 8179, 3729, 17205, 6138, 23327, 6119, 6126, 19649, 19812, 37796, 36557]\n",
      "For paper number 18539, number 12 in the 12-articles set, the recommendations by PageRank are:\n",
      "[7577, 5992, 2143, 10605, 6705, 767, 18561, 1627, 8420, 6708, 18438, 5994, 18440, 9392, 16595, 9396, 18564, 9248, 18550, 18552, 18542, 18541, 5615, 9251, 13675, 18556, 12197, 5613, 16183, 18555, 7574, 18562, 5617, 10604, 8333, 18543, 18013, 18560, 18551, 18540, 5610, 18559, 18544, 18549, 18545, 18547, 7578, 18554, 18563, 18012, 18553, 18546, 18558, 18015, 18557, 18548, 18565, 18011]\n"
     ]
    }
   ],
   "source": [
    "# Create a directed graph of type networkx\n",
    "nx_graph = nx.from_scipy_sparse_matrix(m, create_using=nx.DiGraph)\n",
    "\n",
    "# Convert the networkx graph to type igraph\n",
    "g = ig.Graph.from_networkx(nx_graph)\n",
    "\n",
    "## Calculates PageRank for the graph, using iGraph's standard method\n",
    "pr = g.pagerank()\n",
    "\n",
    "'Gets the indexes of the 12-articles set in the overall matrix'\n",
    "def gets_indexes():\n",
    "    pos = []\n",
    "    for i in articles[\"id\"]:\n",
    "        count = 0\n",
    "        for j in matrix_names[\"x\"]:\n",
    "            if (i==j):\n",
    "                pos.append(count)\n",
    "            count+=1\n",
    "    return pos\n",
    "\n",
    "'Classifies the recommendations of a given index, based on their PageRanks'\n",
    "'i = index to calculate the recommendations'\n",
    "def gets_recommendations(i):\n",
    "    rows, cols = m.tocsr()[i].nonzero()\n",
    "    ranks = np.empty([0, 2])\n",
    "    for j in cols:\n",
    "        ranks = np.vstack([ranks,[j,pr[j]]])\n",
    "    return ranks[ranks[:, 1].argsort()[::-1]]\n",
    "\n",
    "'Gets all the recommendations of the 12 articles set'\n",
    "def gets_articles_recommendations():\n",
    "    pos = gets_indexes()\n",
    "    count = 1\n",
    "    for i in pos:\n",
    "        recs = gets_recommendations(i)\n",
    "        ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset\n",
    "        print(\"For paper number {}, number {} in the 12-articles set, the recommendations by PageRank are:\".format(i+1,count))\n",
    "        vector = recs[:,0].astype(int)\n",
    "        ## Adds one to the recommendations, because index 0 is equivalent to line 1 in the dataset\n",
    "        print([x+1 for x in vector])\n",
    "        count+=1      \n",
    "        \n",
    "gets_articles_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec1cb4",
   "metadata": {},
   "source": [
    "### Q1, Part 2 - Calculate the MRR for the references of each of the 12 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939870cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For paper number 15089, number 1 in the 12-articles set, the MRR is 0.0004574767061335355\n",
      "For paper number 35353, number 2 in the 12-articles set, the MRR is 0.0005500414273253119\n",
      "For paper number 50496, number 3 in the 12-articles set, the MRR is 7.527825807316256e-05\n",
      "For paper number 50497, number 4 in the 12-articles set, the MRR is 0.00036754302748344994\n",
      "For paper number 11636, number 5 in the 12-articles set, the MRR is 0.0669679208682132\n",
      "For paper number 12593, number 6 in the 12-articles set, the MRR is 0.0026283797344994145\n",
      "For paper number 36565, number 7 in the 12-articles set, the MRR is 0.011164408019052923\n",
      "For paper number 12215, number 8 in the 12-articles set, the MRR is 0.002315070782131751\n",
      "For paper number 18645, number 9 in the 12-articles set, the MRR is 0.0020926995056850623\n",
      "For paper number 1594, number 10 in the 12-articles set, the MRR is 0.00202990910701956\n",
      "For paper number 35304, number 11 in the 12-articles set, the MRR is 0.0017984730865547613\n",
      "For paper number 18539, number 12 in the 12-articles set, the MRR is 0.00014504667964216348\n",
      "The overall average MRR is 0.007549353933484524\n",
      "The MRR for the 12 papers was calculated in 15.567795038223267 seconds\n"
     ]
    }
   ],
   "source": [
    "'Creates ordered set of pageranks - the best pagerank will be first'\n",
    "def order_ranks():\n",
    "    ranks = np.empty([0, 2])\n",
    "    for i in range(len(pr)):\n",
    "        ranks = np.vstack([ranks,[i,pr[i]]])\n",
    "    return ranks[ranks[:, 1].argsort()[::-1]]\n",
    "\n",
    "'Given the ordered set, finds the rank of a given index'\n",
    "'i = index to find the rank'\n",
    "'ranks = set of ordered pageranks'\n",
    "def find_rank_index(i, ranks):\n",
    "    count = 1\n",
    "    for j in ranks[:,0]:\n",
    "        if (i==j):\n",
    "            return count\n",
    "        count+=1\n",
    "\n",
    "'Calculates mean reciprocal rank for each of the connections of a given index'\n",
    "'i = index to calculate the MRR'\n",
    "def calculate_mrr(i):\n",
    "    ## Orders the ranks\n",
    "    ranks = order_ranks()\n",
    "    rows, cols = m.tocsr()[i].nonzero()\n",
    "    rrs = []\n",
    "    \n",
    "    ## For each of the references...\n",
    "    for j in cols:\n",
    "        ## Gets the rank of that index in the ordered list...\n",
    "        index = find_rank_index(j,ranks)\n",
    "        ## Calculates the reciprocal rank (1/rank)\n",
    "        rrs.append(1/index)\n",
    "    ## And returns the mean of those values\n",
    "    return(np.mean(rrs))\n",
    "\n",
    "'Calculates MRR for each of the 12 articles'\n",
    "def calculate_mrr_articles():\n",
    "    mrr_vector = []\n",
    "    pos = gets_indexes()\n",
    "    count = 1\n",
    "    for i in pos:\n",
    "        mrr = calculate_mrr(i)\n",
    "        mrr_vector.append(mrr)\n",
    "        ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset\n",
    "        print(\"For paper number {}, number {} in the 12-articles set, the MRR is {}\".format(i+1,count,mrr))\n",
    "        count+=1\n",
    "    print(\"The overall average MRR is {}\".format(np.mean(mrr_vector)))\n",
    "\n",
    "start_time = time.time()\n",
    "calculate_mrr_articles()\n",
    "print(\"The MRR for the 12 papers was calculated in {} seconds\".format((time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016d8a9",
   "metadata": {},
   "source": [
    "### Q2 - Calculate Thematic PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5654ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For paper no. 1 with index 15088, the mrr is:\n",
      "0.23806748806748806\n",
      "For paper no. 2 with index 35352, the mrr is:\n",
      "0.1910424837508171\n",
      "For paper no. 3 with index 50495, the mrr is:\n",
      "0.3888888888888889\n",
      "For paper no. 4 with index 50496, the mrr is:\n",
      "0.5833333333333334\n",
      "For paper no. 5 with index 11635, the mrr is:\n",
      "0.19763121376644732\n",
      "For paper no. 6 with index 12592, the mrr is:\n",
      "0.10990496604102991\n",
      "For paper no. 7 with index 36564, the mrr is:\n",
      "0.21252070152070152\n",
      "For paper no. 8 with index 12214, the mrr is:\n",
      "0.20582681207681208\n",
      "For paper no. 9 with index 18644, the mrr is:\n",
      "0.10827692114544583\n",
      "For paper no. 10 with index 1593, the mrr is:\n",
      "0.22953336346193484\n",
      "For paper no. 11 with index 35303, the mrr is:\n",
      "0.1971857960624611\n",
      "For paper no. 12 with index 18538, the mrr is:\n",
      "0.07621273149462816\n",
      "the average mrr of the 12 articles is: 0.22820205830083237\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "## Loads matrix and CSVs\n",
    "m = scipy.io.mmread('tp-matrix.dgt')\n",
    "articles = pd.read_csv(\"12-articles.csv\", sep=\",\")\n",
    "\n",
    "#convert coo matrix to csr for fast row manipulation\n",
    "csr_type_matrix = m.tocsr()\n",
    "\n",
    "#convert coo matrix to csc for fast column manipulation\n",
    "t_csc_matrix = m.tocsc().transpose()\n",
    "\n",
    "#helper function for get_indices\n",
    "def get_index(id):\n",
    "    for ind in range(len(matrix_names)):\n",
    "            search_id = matrix_names.iloc[ind].values[0]\n",
    "            if search_id == id:\n",
    "                return ind\n",
    "\n",
    "#create array that contains the indexes of the 12 articles \n",
    "def get_indices(articles):\n",
    "    indices = []\n",
    "    for i in range(len(articles)):\n",
    "        target_id = articles.iloc[i].values[0]\n",
    "        indices.append(get_index(target_id))\n",
    "    return indices\n",
    "\n",
    "#get the references' indices of a given article\n",
    "def forward(ind):\n",
    "    return csr_type_matrix[ind].indices\n",
    "\n",
    "#get the citations' indices of a given article\n",
    "def backward(ind):\n",
    "    return t_csc_matrix[ind].indices\n",
    "\n",
    "#create thematique matrix for a given article index\n",
    "def get_thematique_matrix(ind):\n",
    "    row = []\n",
    "    column = []\n",
    "    data = []\n",
    "    #(1)\n",
    "    references = forward(ind)\n",
    "    for x in references:\n",
    "        row.append(ind)\n",
    "        column.append(x)\n",
    "        data.append(1)\n",
    "    #(-1)\n",
    "    citations = backward(ind)\n",
    "    for x in citations:\n",
    "        row.append(x)\n",
    "        column.append(ind)\n",
    "        data.append(1)\n",
    "    #(1, 1) and (1, -1)\n",
    "    for article in references:\n",
    "        references2 = forward(article)\n",
    "        for x in references2:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "        citations2 = backward(article)\n",
    "        for x in citations2:\n",
    "            row.append(x)\n",
    "            column.append(article)\n",
    "            data.append(1)\n",
    "    #(-1, 1)\n",
    "    for article in citations:\n",
    "        references3 = forward(article)\n",
    "        for x in references3:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "    #(1, -1, 1)\n",
    "    if len(references) > 0:\n",
    "        for article in citations2:\n",
    "            references4 = forward(article)\n",
    "            for x in references4:\n",
    "                row.append(article)\n",
    "                column.append(x)\n",
    "                data.append(1)\n",
    "    #(-1, 1, 1)\n",
    "    if len(citations) > 0:\n",
    "        for article in references3:\n",
    "            references5 = forward(article)\n",
    "            for x in references5:\n",
    "                row.append(article)\n",
    "                column.append(x)\n",
    "                data.append(1)\n",
    "    #convert the three arrays into a single sparse matrix\n",
    "    result = csr_matrix((data, (row, column)))\n",
    "    #make the data values all ones\n",
    "    result.data = np.ones(len(result.data), dtype=int)\n",
    "    return result\n",
    "\n",
    "def thematique_mrr(ind):\n",
    "    thematique_matrix = get_thematique_matrix(ind)\n",
    "    references = forward(ind)\n",
    "    MRRs = []\n",
    "    for ref in references:\n",
    "        #create graph\n",
    "        sources, targets = thematique_matrix.nonzero()\n",
    "        edgelist = zip(sources.tolist(), targets.tolist())\n",
    "        them_graph = ig.Graph(edges = list(edgelist), directed = True)\n",
    "        them_graph.delete_edges(them_graph.get_eid(ind, ref))\n",
    "        #calculate pagerank\n",
    "        pr = ig.Graph.pagerank(them_graph, directed = True, damping = 0.5)\n",
    "        new_pr = []\n",
    "        new_indices = []\n",
    "        for i in range(len(pr)):\n",
    "            if i in sources or i in targets:\n",
    "                new_indices.append(i)\n",
    "                new_pr.append(pr[i])\n",
    "        #order the articles according to the pagerank\n",
    "        ordered_indices = [x for _,x in sorted(zip(new_pr, new_indices), reverse = True)]\n",
    "        #get the rank of the left out article\n",
    "        a = ordered_indices.index(ref) + 1\n",
    "        MRRs.append(1/a)\n",
    "        #restore the link\n",
    "        thematique_matrix[ind, ref] = 1\n",
    "    return np.mean(MRRs)\n",
    "\n",
    "def mrr(articles):\n",
    "    indices = get_indices(articles)\n",
    "    avg_mrr = []\n",
    "    for i in range(len(indices)):\n",
    "        mrr = thematique_mrr(indices[i])\n",
    "        avg_mrr.append(mrr)\n",
    "        print(\"For paper no. {} with index {}, the mrr is:\\n{}\".format(i+1, indices[i], mrr))\n",
    "    print(\"the average mrr of the 12 articles is: {}\".format(np.mean(avg_mrr)))\n",
    "\n",
    "mrr(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c34a16",
   "metadata": {},
   "source": [
    "### Q3, Part 1 - Get 10 recommendations by cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb471fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 cosine similarities for paper 15089 are:\n",
      "[17446, 19394, 17447, 25491, 36749, 30969, 28608, 26119, 18172, 16044]\n",
      "The top-10 cosine similarities for paper 35353 are:\n",
      "[39168, 45977, 43584, 24611, 26750, 41630, 36319, 41641, 25419, 34287]\n",
      "The top-10 cosine similarities for paper 50496 are:\n",
      "[41644, 7749, 4800, 40750]\n",
      "The top-10 cosine similarities for paper 50497 are:\n",
      "[7732, 31005, 14111, 16689, 6760, 27413, 95, 28775, 38855, 1193]\n",
      "The top-10 cosine similarities for paper 11636 are:\n",
      "[11635, 19953, 43669, 43054, 49204, 3476, 46807, 2076, 45027, 30700]\n",
      "The top-10 cosine similarities for paper 12593 are:\n",
      "[34207, 42260, 18242, 40537, 42347, 28864, 30952, 9527, 39934, 40483]\n",
      "The top-10 cosine similarities for paper 36565 are:\n",
      "[37444, 30419, 37199, 28067, 42417, 8706, 12063, 22036, 29061, 2238]\n",
      "The top-10 cosine similarities for paper 12215 are:\n",
      "[6711, 17348, 20283, 17675, 12705, 13037, 20023, 26298, 19242, 8286]\n",
      "The top-10 cosine similarities for paper 18645 are:\n",
      "[31963, 159, 32474, 10237, 21327, 32473, 4084, 43377, 6745, 48483]\n",
      "The top-10 cosine similarities for paper 1594 are:\n",
      "[33116, 19336, 37445, 31570, 9310, 9967, 34418, 37363, 40760, 22500]\n",
      "The top-10 cosine similarities for paper 35304 are:\n",
      "[46425, 49445, 26292, 32872, 6118, 37084, 7740, 27161, 27507, 47931]\n",
      "The top-10 cosine similarities for paper 18539 are:\n",
      "[32959, 18548, 18563, 44023, 30931, 29062, 33262, 18541, 24500, 48253]\n"
     ]
    }
   ],
   "source": [
    "'Method to get the cosine similarities of one given index in a matrix. It does not transpose the matrix to do so.'\n",
    "'index = the index of the paper to calculate the similarities'\n",
    "'matrix = the adjacency matrix'\n",
    "'n = the number of top similarities to return'\n",
    "def gets_cosine_sim(index, matrix, n):\n",
    "    \n",
    "    ## Calculates cosine similarity matrix and the values of the given index\n",
    "    cos_mat = cosine_similarity(matrix, dense_output=False)\n",
    "    idx = cos_mat[index]\n",
    "    \n",
    "    ## Gets the index's values as a COO matrix\n",
    "    idx = idx.tocoo()    \n",
    "    ranks = np.empty([0, 2])\n",
    "    \n",
    "    ## Stacks the values in a numpy array to order and get the N highest values\n",
    "    for _,row,sim in zip(idx.row, idx.col, idx.data):\n",
    "        ranks = np.vstack([ranks,[row,sim]])    \n",
    "    ranks = ranks[ranks[:, 1].argsort()[::-1]]\n",
    "    \n",
    "    ## LEAVE ONE OUT - Gets the n+1 values, since the first value will always be the index itself (1.0 cosine similarity)\n",
    "    vector = ranks[:,0][:n+1].astype(int)\n",
    "    \n",
    "    ## For the elements with no similarities calculated (e.g., index 15089), returns null\n",
    "    if (len(vector) == 0):\n",
    "        print(\"No cosine similarities detected.\")\n",
    "        return\n",
    "    \n",
    "    ## Removes the first value\n",
    "    vector = np.delete(vector, 0)\n",
    "\n",
    "    return vector\n",
    "   \n",
    "'Calculates cosine similarities for the 12-articles dataset'\n",
    "'n = selects only top-N similarities'\n",
    "'m = matrix'\n",
    "def get_similarities(m, n): \n",
    "    pos = gets_indexes()\n",
    "    count = 1\n",
    "    for i in pos:\n",
    "        vector = gets_cosine_sim(i, m, n)            \n",
    "        ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset\n",
    "        print(\"The top-{} cosine similarities for paper {} are:\".format(n,i+1))\n",
    "        ## Adds one to the recommendations, because index 0 is equivalent to line 1 in the dataset\n",
    "        print([x+1 for x in vector])\n",
    "    \n",
    "get_similarities(m, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a36e1af",
   "metadata": {},
   "source": [
    "### Q3, Part 2 - Calculate the MRR for the cosine similarities of each of the 12 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43212b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For paper number 15089, number 1 in the 12-articles set, the MRR is 5.4055492423757996e-05\n",
      "For paper number 35353, number 2 in the 12-articles set, the MRR is 4.506482488241559e-05\n",
      "For paper number 50496, number 3 in the 12-articles set, the MRR is 3.852826281582077e-05\n",
      "For paper number 50497, number 4 in the 12-articles set, the MRR is 6.384566636101523e-05\n",
      "For paper number 11636, number 5 in the 12-articles set, the MRR is 5.176402281003114e-05\n",
      "For paper number 12593, number 6 in the 12-articles set, the MRR is 6.281793429907787e-05\n",
      "For paper number 36565, number 7 in the 12-articles set, the MRR is 6.22830461807635e-05\n",
      "For paper number 12215, number 8 in the 12-articles set, the MRR is 8.028286343709515e-05\n",
      "For paper number 18645, number 9 in the 12-articles set, the MRR is 0.0031358947267151626\n",
      "For paper number 1594, number 10 in the 12-articles set, the MRR is 0.00010947977957715294\n",
      "For paper number 35304, number 11 in the 12-articles set, the MRR is 3.884581931922746e-05\n",
      "For paper number 18539, number 12 in the 12-articles set, the MRR is 5.943950177031666e-05\n",
      "The overall average MRR is 0.0003168584950493198\n"
     ]
    }
   ],
   "source": [
    "'Calculates mean reciprocal rank for each of the connections of a given index'\n",
    "'i = index to calculate the MRR'\n",
    "def calculate_mrr_cosine(i):\n",
    "    ## Orders the ranks\n",
    "    ranks = order_ranks()\n",
    "    sims = gets_cosine_sim(i,m,10)\n",
    "    rrs = []\n",
    "    \n",
    "    ## For each of the references...\n",
    "    for j in sims:\n",
    "        ## Gets the rank of that index in the ordered list...\n",
    "        index = find_rank_index(j,ranks)\n",
    "        ## Calculates the reciprocal rank (1/rank)\n",
    "        rrs.append(1/index)\n",
    "    ## And returns the mean of those values\n",
    "    return(np.mean(rrs))\n",
    "\n",
    "'Calculates MRR for each of the 12 articles'\n",
    "def calculate_mrr_articles_cosine():\n",
    "    mrr_vector = []\n",
    "    pos = gets_indexes()\n",
    "    count = 1\n",
    "    for i in pos:\n",
    "        mrr = calculate_mrr_cosine(i)\n",
    "        mrr_vector.append(mrr)\n",
    "        ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset\n",
    "        print(\"For paper number {}, number {} in the 12-articles set, the MRR is {}\".format(i+1,count,mrr))\n",
    "        count+=1\n",
    "    print(\"The overall average MRR is {}\".format(np.mean(mrr_vector)))\n",
    "    \n",
    "calculate_mrr_articles_cosine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96d23b",
   "metadata": {},
   "source": [
    "### Q3, Part 3 - Get 10 recommendations by cosine similarity in the thematic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424258a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 cosine similarities for paper 15089 are:\n",
      "[19394, 25491, 30969, 18173, 24481, 3262, 26119, 16044, 28608, 7252]\n",
      "The top-10 cosine similarities for paper 35353 are:\n",
      "[14777, 48763, 41791, 31286, 20905, 36486, 41631, 45475, 10631, 30626]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#convert coo matrix to csr for fast row manipulation\n",
    "csr_type_matrix = m.tocsr()\n",
    "\n",
    "#convert coo matrix to csc for fast column manipulation\n",
    "t_csc_matrix = m.tocsc().transpose()\n",
    "\n",
    "#get the references' indices of a given article\n",
    "def forward(ind):\n",
    "    return csr_type_matrix[ind].indices\n",
    "\n",
    "#get the citations' indices of a given article\n",
    "def backward(ind):\n",
    "    return t_csc_matrix[ind].indices\n",
    "\n",
    "#create thematique matrix for a given article index\n",
    "def get_thematique_matrix(ind):\n",
    "    row = []\n",
    "    column = []\n",
    "    data = []\n",
    "    #(1)\n",
    "    references = forward(ind)\n",
    "    for x in references:\n",
    "        row.append(ind)\n",
    "        column.append(x)\n",
    "        data.append(1)\n",
    "    #(-1)\n",
    "    citations = backward(ind)\n",
    "    for x in citations:\n",
    "        row.append(x)\n",
    "        column.append(ind)\n",
    "        data.append(1)\n",
    "    #(1, 1) and (1, -1)\n",
    "    for article in references:\n",
    "        references2 = forward(article)\n",
    "        for x in references2:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "        citations2 = backward(article)\n",
    "        for x in citations2:\n",
    "            row.append(x)\n",
    "            column.append(article)\n",
    "            data.append(1)\n",
    "    #(-1, 1)\n",
    "    for article in citations:\n",
    "        references3 = forward(article)\n",
    "        for x in references3:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "    #(1, -1, 1)\n",
    "    for article in citations2:\n",
    "        references4 = forward(article)\n",
    "        for x in references4:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "    #(-1, 1, 1)\n",
    "    for article in references3:\n",
    "        references5 = forward(article)\n",
    "        for x in references5:\n",
    "            row.append(article)\n",
    "            column.append(x)\n",
    "            data.append(1)\n",
    "    \n",
    "    #convert the three arrays into a single sparse matrix\n",
    "    result = csr_matrix((data, (row, column)))\n",
    "    #make the data values all ones\n",
    "    result.data = np.ones(len(result.data), dtype=int)\n",
    "    return result\n",
    "\n",
    "'Calculates cosine similarities for the 12-articles dataset'\n",
    "'i = index of the paper'\n",
    "'n = selects only top-N similarities'\n",
    "def get_similarities_thematic(i,n):\n",
    "    m_them = get_thematique_matrix(i)\n",
    "    vector = gets_cosine_sim(i, m_them, n)            \n",
    "    ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset\n",
    "    print(\"The top-{} cosine similarities for paper {} are:\".format(n,i+1))\n",
    "    ## Adds one to the recommendations, because index 0 is equivalent to line 1 in the dataset\n",
    "    print([x+1 for x in vector])\n",
    "\n",
    "get_similarities_thematic(15088, 10)\n",
    "get_similarities_thematic(35352, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b8733",
   "metadata": {},
   "source": [
    "### Q3, Part 4 - Calculate the MRR for the cosine similarities of each of the 12 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656cd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For paper number 15089, the thematic MRR is 9.331697581306812e-05\n",
      "For paper number 35353, the thematic MRR is 4.3218873290918074e-05\n"
     ]
    }
   ],
   "source": [
    "'Calculates mean reciprocal rank for each of the connections of a given index'\n",
    "'i = index to calculate the MRR'\n",
    "def calculate_mrr_cosine_thematic(i):\n",
    "    ## Orders the ranks\n",
    "    m_them = get_thematique_matrix(i)\n",
    "    ranks = order_ranks()\n",
    "    sims = gets_cosine_sim(i,m_them,10)\n",
    "    rrs = []\n",
    "    \n",
    "    ## For each of the references...\n",
    "    for j in sims:\n",
    "        ## Gets the rank of that index in the ordered list...\n",
    "        index = find_rank_index(j,ranks)\n",
    "        ## Calculates the reciprocal rank (1/rank)\n",
    "        rrs.append(1/index)\n",
    "        \n",
    "    ## Adds one to variable i, because index 0 is equivalent to line 1 in the dataset, and prints the mean of those values\n",
    "    print(\"For paper number {}, the thematic MRR is {}\".format(i+1,np.mean(rrs)))\n",
    "    \n",
    "calculate_mrr_cosine_thematic(15088)\n",
    "calculate_mrr_cosine_thematic(35352)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0cea7e",
   "metadata": {},
   "source": [
    "### Q4, Part 1 - Personalized PageRank - We pre-process the text from the abstracts CSV and generate embeddings from it using a Transformers model. Then, we calculate a cosine similarity matrix between all the embeddings. The idea is to find the texts that are more similar to the others in the dataset, and give those papers more importance in our calculations.\n",
    "### Then, we calculate a \"similarity coefficient\", the average of all cosine similarity that a paper has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86d7fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "'Tokenizes each word of a sentence'\n",
    "'sentences = set of sentences'\n",
    "def tokenize_sentences(sentences):\n",
    "    return (list(map(str.split, sentences)))\n",
    "\n",
    "'Removes stopwords, lowers the words, and removes words with non-alpha chars from the set'\n",
    "'sentences = set of sentences'\n",
    "def clean_sentences(sentences): \n",
    "    sw = stopwords.words('english') \n",
    "    sentence_set = sentences\n",
    "    for i in range(len(sentence_set)):\n",
    "        prev_sentence = sentence_set[i]\n",
    "        new_sentence = []\n",
    "        for word in prev_sentence:\n",
    "            if word not in sw and word.isalpha():\n",
    "                new_sentence.append(word.lower())\n",
    "        sentence_set[i] = ' '.join(new_sentence)\n",
    "    return sentence_set\n",
    "\n",
    "'Generates embeddings from the abstracts, using a Transformer model'\n",
    "def generate_abstract_embeddings():    \n",
    "    ## Reads the abstracts CSV\n",
    "    matrix_abstracts = pd.read_csv(\"tp-abstracts.csv\", sep=\",\")\n",
    "    raw_abstracts = matrix_abstracts[\"x\"]\n",
    "    \n",
    "    ## Fills NaN with a blank token\n",
    "    raw_abstracts = raw_abstracts.fillna(\"\")\n",
    "    \n",
    "    ## Tokenizes and cleans sentences \n",
    "    abstracts = tokenize_sentences(raw_abstracts)\n",
    "    abstracts = clean_sentences(abstracts)\n",
    "    \n",
    "    ## Creates Transformer model, and converts the abstracts to embeddings\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    ##embeddings = model.encode(abstracts)\n",
    "    ## Saves the file, to avoid repetition of the embeddings creation (optional step)\n",
    "    #np.save(\"embeddings.npy\", embeddings)\n",
    "    embeddings = np.load(\"embeddings.npy\")\n",
    "    return embeddings\n",
    "\n",
    "'Calculates the cosine similarity matrix between all embeddings. Then, calculates the average similarity for each row.'\n",
    "'embeddings = set of sentence embeddings'\n",
    "def calculate_similarity(embeddings):\n",
    "    ## Converts the embeddings to a sparse matrix\n",
    "    embeddings_sparse = sparse.csr_matrix(embeddings)\n",
    "    \n",
    "    ## Calculates a cosine similarity sparse matrix for the embeddings - takes a VERY LONG TIME\n",
    "    cos_mat = cosine_similarity(sentence_embeddings, dense_output=False)\n",
    "    \n",
    "    ## Calculates average similarity for each of the rows\n",
    "    avg_cos_mat = []\n",
    "    for i in range(cos_mat.shape[0]):\n",
    "        avg = np.mean(cos_mat[i])\n",
    "        avg_cos_mat.append(avg)\n",
    "    \n",
    "    ## Saves file\n",
    "    np.save(\"avg_cos_mat.npy\", avg_cos_mat)\n",
    "    \n",
    "    return avg_cos_mat\n",
    "\n",
    "#embeddings = generate_abstract_embeddings()\n",
    "#avg_cos_mat = calculate_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c7c446",
   "metadata": {},
   "source": [
    "### Q4 - Here is an example of processed tokens, and their respective embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2709181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a fundamental problem text data mining extract meaningful structure document streams arrive continuously news articles two natural examples characterized topics grow intensity period fade the published literature particular research field seen exhibit similar phenomena much longer time underlying much text mining work area following intuitive premise appearance topic document stream signaled certain features rising sharply frequency topic goal present work develop formal approach modeling way robustly efficiently provide organizational framework analyzing underlying the approach based modeling stream using bursts appear naturally state viewed drawing analogy models queueing theory bursty network the resulting algorithms highly yield nested representation set bursts imposes hierarchical structure overall experiments research paper archives suggest resulting structures natural meaning terms content gave rise\n",
      "[-0.15821324  0.48515302  1.0967705   0.29859713  0.04373763 -1.3476443\n",
      " -0.3851786   0.02048862  0.19333537 -0.39360958 -0.18301237  0.9828511\n",
      " -0.01458709  0.27074036 -0.21452719  0.22621936 -0.6219196  -0.5885133\n",
      "  0.32434845 -0.41537124 -0.0407869  -0.6367915   0.41297656  0.07020741\n",
      "  1.3236818   0.12318537 -0.41373098 -0.01497658 -1.0570059   0.32476696\n",
      " -0.81323475  0.30943877 -0.2133357  -0.14966    -0.43338332  1.0540742\n",
      "  0.63686246  0.16483788  0.46528074 -0.86662817  0.4109901  -0.07447385\n",
      "  0.09181648 -0.3839385  -1.3005068  -0.21937257 -1.2573344   0.0382361\n",
      "  0.06926861 -0.48972765 -0.43901214  1.054561    0.34977213 -0.33732477\n",
      " -0.40077385  0.47818145  0.1819224  -0.972564   -0.39291373 -0.2243367\n",
      " -0.41116384 -0.11900394 -0.24658956  0.4498517  -0.386325    0.22399071\n",
      "  0.33178777 -0.718599   -1.1986114  -0.60032254  0.36279482 -0.7793141\n",
      " -0.54262424 -0.09706728 -0.30852735  0.22182192 -0.06334276  0.07278312\n",
      "  0.3715714   0.8365241  -0.73102194  0.02826452  0.52734214 -0.06013108\n",
      "  0.15028322  0.12995254  0.8331666   0.39023763 -0.5168998   0.9477213\n",
      "  0.33646807 -0.36823544  0.81140447 -0.5962881  -0.07516688 -0.37636957\n",
      " -0.07092042 -0.07429183 -0.4058154   0.1534575  -1.3847953  -0.3864162\n",
      " -0.08874435  0.3609481  -0.8004384   0.67480266 -0.32670745 -0.11568943\n",
      " -0.02789631  0.08150596  1.2101297  -0.14490515 -0.6888734  -0.10780013\n",
      "  0.3662097  -0.45406353 -0.5886984   1.5273736   0.88745874  0.04582551\n",
      " -0.07949699 -0.2701553   0.01208927  0.0458744   0.30207413  0.1439319\n",
      "  0.8774504   0.09212696  0.25171122  0.04358784 -0.17413104  1.0167191\n",
      "  0.19363378 -0.04541911  0.07014545  0.65852207  0.35636562 -0.43464833\n",
      "  0.13048926 -0.54800564 -0.32296878 -0.10415383  0.38740888 -0.41548747\n",
      " -0.39489323 -0.56795144 -0.02794433 -1.0732155   0.17216761  0.07623117\n",
      " -0.35652873  0.69657946 -1.7064996   0.49974376  0.29933703  0.36674476\n",
      " -0.32213253 -0.18894565 -0.13523068  0.30974942 -0.5297627   1.3410076\n",
      " -0.2654727   0.35809767  0.47333884 -1.1412046   0.5431532   0.03407409\n",
      " -0.03679682 -0.46859467 -0.43194386  0.78242666  1.540496    0.35755947\n",
      "  0.2073711   0.5518069   1.4469924  -0.43479708 -0.2549252  -0.54762876\n",
      " -0.02598278  0.57053393 -0.5319622   0.6705655   0.7532469  -0.56064016\n",
      "  1.4349946  -0.22444266  0.435741   -0.53174     0.00655619 -0.06902465\n",
      " -0.10085811 -0.6730301   0.21004362  0.82916707 -0.41370815  1.627451\n",
      "  0.5463858   0.35226688  0.03209795  0.6108661   0.55223405 -1.2816206\n",
      " -0.14928018 -0.17741805 -0.05126543 -0.09621675 -1.3227135   0.26073727\n",
      " -0.39328197  1.0189031   0.97761756 -0.092213    0.3355756   0.01061534\n",
      "  0.37015918 -0.0490974   0.3961155   0.19571236  0.61381036 -0.07455424\n",
      " -0.62913907  1.2632704  -0.159147    0.42005354  0.845227   -0.6267183\n",
      "  0.5093831   0.03321182  0.09205583 -0.13233523 -0.1907891   0.09547928\n",
      "  0.5055014   0.30515006 -0.40261963  0.74697936  0.5674751  -0.01238626\n",
      "  0.7126271   1.4208812   0.3919974   1.0408348  -0.6181435  -0.36645105\n",
      "  0.6214584   0.00321582 -0.4805435  -0.3992446  -0.57100815  0.15975812\n",
      "  0.02370198 -0.81954736  0.12363522 -0.31614104 -0.31541905 -0.4012254\n",
      " -0.39804098  0.09818998 -0.01773961 -0.4038693  -0.567953    0.5106324\n",
      "  0.17836463 -0.23789212  0.05616141 -0.686352   -0.1635554   0.18137363\n",
      " -0.79720914 -0.3503636   0.02865112  0.08376942  0.21099982 -0.6661382\n",
      " -0.05697131 -0.04018234  0.46281376  0.42975566 -0.35054561  0.5885565\n",
      " -0.690099   -0.3174929  -0.27484882 -0.04196638  0.12899216 -0.27411354\n",
      " -0.17693584  0.12681952 -1.149822   -0.52165794 -0.4394404  -0.5185147\n",
      "  0.40039682 -0.14164317  1.1000645  -0.5733701   0.15424728 -0.31319985\n",
      " -0.7199022   0.5119016  -0.9730664   0.12780887 -0.21711028 -0.52253455\n",
      " -0.11164762  0.93023694 -1.5139877  -0.3534224  -0.4116079  -1.2334409\n",
      " -0.61457276  0.4480123  -0.5792704  -0.05452583 -0.99555546 -0.6057539\n",
      "  0.09047286 -0.5878127  -0.6587901   0.35085687  0.22001916 -0.2855296\n",
      "  0.16470529  0.23387855  0.04347449  0.09454093 -0.31769994 -0.16302735\n",
      "  0.9980171  -0.53452843  0.4704522   0.9149094  -0.05162752  0.8559886\n",
      " -0.7695184  -0.1621873   0.5245994  -0.37209386  0.9895741   0.33510122\n",
      "  0.01721077 -0.09950391  0.5907734  -0.15034473  0.47028384 -0.12979631\n",
      "  0.00921349 -0.42274836  0.35820982 -0.2864785   0.280706   -0.41610125\n",
      " -0.16060251  0.5982993   0.19056825  0.3068512  -0.8126651   0.19598074\n",
      " -0.3068428   0.23379731 -0.12588957 -0.05756786 -0.12887263 -0.13455705\n",
      " -0.04645369 -0.29522997  0.07024799 -0.28323933  0.05945987  0.588898\n",
      " -0.4160112   0.01497934 -0.30262563 -0.25900993 -0.5013063  -0.10565288\n",
      " -0.10496392  0.07687648 -0.00950838 -0.43498626 -0.09439222 -0.44925386\n",
      "  0.6037832   0.361382    0.12327988  0.49526867 -0.02935832 -0.2670363\n",
      "  0.14238073  0.5680908   0.35141203 -0.67958236  0.05216901 -0.93294686\n",
      " -0.49597907 -0.3467081   0.06717213 -0.8108532  -0.22277847  0.38488567\n",
      "  1.3602118  -1.2699281   0.03488711  0.5959119   0.56928927 -0.81973165\n",
      " -0.11010634 -0.9563193  -0.5888493  -0.7221899   0.95139474  0.04766644\n",
      " -0.65983737  0.45027798  0.08277711 -0.3379291  -0.80822366  1.1760542\n",
      " -0.2992718   0.4135935   0.6568491  -0.26326668  0.06088017 -0.01466545\n",
      " -0.01214661 -0.01543054 -0.26014853  0.39367756 -0.00275731 -0.16462466\n",
      "  0.19723828  1.2561939   0.8603757  -0.21372235  0.6888206  -1.3447016\n",
      "  0.20102029 -1.2093593   0.26273593 -0.99138486  0.794049   -0.6511573\n",
      " -0.6177965   1.1248196   0.33340254  0.7300751   0.51965797 -0.0515483\n",
      "  0.51836854 -0.47655648  0.20142502  0.49634615  0.17375076  0.52462393\n",
      " -0.14061111 -1.0323461  -1.2551612  -0.37399256  0.05940606 -0.38851595\n",
      "  0.20390686 -0.49564183  0.2266245   0.84933156 -0.10582865 -0.7203445\n",
      "  0.10793523 -0.13074298  0.0115055  -0.57395303  0.31712604  1.0838916\n",
      " -0.0718651   0.06118471 -1.2251086   0.5490671  -0.32347247  0.31484854\n",
      " -0.6202255   0.14730123  0.1291517  -0.08789915 -0.01478895 -0.37801287\n",
      "  0.41614902  1.2161428   0.09574146 -0.20300531  0.5431698   0.36647508\n",
      " -0.24328794 -0.54169804 -0.17832287  0.22997998 -0.33688474  0.7464803\n",
      "  0.91280794  0.3297885   0.7112149   0.27209863  0.13624962 -0.6885115\n",
      "  0.5290738   0.38613495  0.0727101  -0.5201061   0.28347328  0.0725919\n",
      " -0.06282026 -0.24430515  0.02750449 -0.1720978  -0.71524     0.6062973\n",
      " -0.16482763 -0.01178512 -0.5258573  -0.6248061  -0.09225227  0.5649046\n",
      "  0.9243458  -1.0145683  -0.46422446  0.388       0.46164963  0.33588037\n",
      "  1.0047935  -0.16060254 -0.16083911  0.2075838   0.33635432  0.53977793\n",
      "  0.06288073 -0.50321347 -0.27495238 -0.30965027  0.664875   -0.60433763\n",
      "  0.15976113  0.2734731   0.5131539  -0.74880666 -0.46625873  0.43731585\n",
      "  0.11793591 -0.97093326 -0.34072876  0.14389358 -0.8629834  -0.08744708\n",
      "  0.59877026  0.90171087  0.28831494  0.51787597 -1.0719441   0.42407635\n",
      " -0.2949484   0.91706884  0.05690168  0.7124386  -0.49367404 -0.21421117\n",
      " -0.24152066 -0.4255422  -0.4335979  -0.50227445 -0.6906545   0.08685789\n",
      "  0.3426493  -0.19341707  1.3368608   1.4016005  -1.0865977  -0.23755407\n",
      "  0.32363307 -0.5423656   0.15367118 -0.24767685 -0.71070397  0.75052357\n",
      " -0.17650555  0.40705436 -0.7323132  -0.05389194  0.46693856  0.66757166\n",
      " -0.24058196 -0.5589725  -0.66113317 -0.6052634   0.05312847  0.7568491\n",
      "  0.00997582  0.01600167  0.57896227  0.5730967   0.10716847 -0.25282204\n",
      " -0.07935786 -0.8418281  -0.81145245 -0.00292006  0.06045466 -0.08612786\n",
      " -0.8997644   0.12159681 -0.38765672 -0.29247886  0.7933653  -1.2900542\n",
      "  0.77642983 -0.45895335 -0.33313817  0.12593962 -0.15400289 -0.12862974\n",
      "  0.30825353  0.09943292 -0.10625815  0.54898345 -0.32843605 -0.7077562\n",
      "  0.5606445  -0.6326923   0.5922065  -0.0533329  -0.47983092  0.40600863\n",
      "  0.22272211  0.23410283 -0.13296062  0.7043507  -0.19448164  1.0195376\n",
      " -1.0437865   0.2919371   0.36299133  0.7631823  -1.7422895   0.39712882\n",
      " -0.60744923 -0.9378936   0.61795926 -0.33057088  0.474922    0.3093025\n",
      "  1.1821698   0.10961415  0.4433738   0.94712704 -0.5448162   0.07304309\n",
      "  0.28833038 -0.33867258 -0.1517963   0.1821611   0.3580176   1.0522311\n",
      "  0.21201965 -0.38979757  0.62129325 -0.55708104  0.42589253 -0.02543747\n",
      "  0.252982   -0.34566152  0.23081546  0.11291797 -0.10077865  0.01966054\n",
      "  0.5418685  -0.56782633  1.1585697   0.31074116  0.47036165  0.10325325\n",
      " -0.0545295  -1.6136922  -0.160002   -0.94595504 -0.1714326  -1.0638446\n",
      "  0.33656085  0.05369298 -0.23220418  0.31476077 -0.73621225  1.2940638\n",
      "  0.77208805  0.05364751 -0.17365068 -0.7287913   0.35255885 -0.10956404\n",
      " -0.19968683  0.00870979  0.32642844  0.10891359 -1.4103397   0.84851325\n",
      " -1.0614208  -0.15334056  0.23989576  0.30619085 -0.50116056  0.69065255\n",
      " -0.21973279 -0.1458754   0.07147626  0.56778747 -0.3401748   0.19403145\n",
      " -0.748398    0.20876588  0.11265519 -0.01615453 -0.134148    0.5066509\n",
      " -0.32023162  0.19182351 -0.7237066  -1.4678718  -0.38745743 -0.26164347\n",
      "  0.8591823   0.2663947  -0.9600608  -0.083976    0.9785697  -0.61236894\n",
      " -0.27688822  0.1682706  -0.15409799 -0.03767626  0.13420676 -0.6459321\n",
      " -1.1583515  -0.45432493  0.05289416 -0.35336253 -0.35418373 -0.23819302\n",
      " -0.21397012  0.14580923  0.15494296  0.4018394  -0.13081382  0.4862695\n",
      " -0.0474692  -0.8642117   0.11283538  0.8629706   0.1883247   0.34960032\n",
      " -0.26697442 -0.30883947  0.02728594 -0.34963396 -0.87818736 -1.2976187\n",
      " -0.27820125 -0.8734569   0.07995612 -0.05668194  0.07078768  0.03001917]\n"
     ]
    }
   ],
   "source": [
    "'Example of processed tokens, and their respective embeddings'\n",
    "def generate_examples():\n",
    "    matrix_abstracts = pd.read_csv(\"tp-abstracts.csv\", sep=\",\")\n",
    "    raw_abstracts = matrix_abstracts[\"x\"]\n",
    "    \n",
    "    ## Fills NaN with a blank token\n",
    "    raw_abstracts = raw_abstracts.fillna(\"\")\n",
    "    \n",
    "    ## Tokenizes and cleans sentences \n",
    "    abstracts = tokenize_sentences(raw_abstracts)\n",
    "    abstracts = clean_sentences(abstracts)\n",
    "    \n",
    "    ## Loads embeddings file\n",
    "    embeddings = np.load(\"embeddings.npy\")\n",
    "    \n",
    "    print(abstracts[2])\n",
    "    print(embeddings[2])\n",
    "\n",
    "generate_examples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb838a4",
   "metadata": {},
   "source": [
    "### Q4, Part 2 - Calculate Personalized PageRank - We will calculate a new PageRank, based on the equation below: PR_i is the regular PageRank, beta is a weight balance, and the mean of similarities is the value calculated in the previous cell. "
   ]
  },
  {
   "attachments": {
    "ppr.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "40d2ef21",
   "metadata": {},
   "source": [
    "![ppr.jpg](attachment:ppr.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e12ab517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest PageRanks for the Regular PageRank are:\n",
      "Paper 20866, with PageRank 0.0019949398323845656\n",
      "Paper 49760, with PageRank 0.0018904149892596516\n",
      "Paper 50196, with PageRank 0.0016410131705481502\n",
      "Paper 49932, with PageRank 0.0014244703063669528\n",
      "Paper 49933, with PageRank 0.001405662848934266\n",
      "Paper 50095, with PageRank 0.0010952413229400396\n",
      "Paper 50096, with PageRank 0.0010915973731974481\n",
      "Paper 20865, with PageRank 0.000753942561226734\n",
      "Paper 48253, with PageRank 0.0006993949247889471\n",
      "Paper 50292, with PageRank 0.000625064464484425\n",
      "\n",
      "\n",
      "The highest PageRanks for the Personalized PageRank are:\n",
      "Paper 35406, with PageRank 0.7281211377718421\n",
      "Paper 45264, with PageRank 0.7279705322017386\n",
      "Paper 45243, with PageRank 0.7276162036647819\n",
      "Paper 37438, with PageRank 0.7271217069188506\n",
      "Paper 3883, with PageRank 0.7246983438300684\n",
      "Paper 26502, with PageRank 0.724197723752468\n",
      "Paper 34881, with PageRank 0.7234532653964901\n",
      "Paper 20297, with PageRank 0.7225400426620592\n",
      "Paper 19114, with PageRank 0.7222921070947529\n",
      "Paper 29948, with PageRank 0.7222763774359007\n"
     ]
    }
   ],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "\n",
    "'Calculates the personalized pageranks, based on the equation above'\n",
    "def generates_personalized_pagerank():\n",
    "    ## Loads iGraph file\n",
    "    g = ig.Graph.Read_GraphML(\"graph.gra\", index=0)\n",
    "    ## Calculates PageRank for the graph, using iGraph's standard method\n",
    "    pr = g.pagerank()\n",
    "    ## Loads similarity coefficients\n",
    "    avg_cos_mat = np.load(\"avg_cos_mat.npy\")\n",
    "    ## Weight factor - we will use the same weight factor as used in the Atelier Personalized PageRank, beta = 3\n",
    "    ## This value can be modified to increase or decrease the importance of the average similarity of the papers\n",
    "    beta = 3\n",
    "    ## Calculates the new PageRanks\n",
    "    ppr = []\n",
    "    for i in range(len(pr)):\n",
    "        ppr_i = (pr[i] + beta * avg_cos_mat[i]) / beta\n",
    "        ppr.append(ppr_i)\n",
    "    return ppr\n",
    "\n",
    "'Creates ordered set of personalized pageranks - the best pagerank will be first'\n",
    "'ppr = personalized pagerank list'\n",
    "def order_ranks(ppr):\n",
    "    ranks = np.empty([0, 2])\n",
    "    for i in range(len(ppr)):\n",
    "        ranks = np.vstack([ranks,[i,ppr[i]]])\n",
    "    return ranks[ranks[:, 1].argsort()[::-1]]\n",
    "\n",
    "'Prints the top PageRanks from the regular method, and from our Personalized method'\n",
    "'pr = regular pagerank list'\n",
    "'ppr = personalized pagerank list'\n",
    "'n = top N items to get'\n",
    "def gets_top_ranks(pr, ppr, n):\n",
    "    top_pr = order_ranks(pr)[0:n]\n",
    "    top_ppr = order_ranks(ppr)[0:n]\n",
    "    print(\"The highest PageRanks for the Regular PageRank are:\")    \n",
    "    ## Adds one to the papers, because index 0 is equivalent to line 1 in the dataset/\n",
    "    for i,j in top_pr:\n",
    "        print(\"Paper {}, with PageRank {}\".format(int(i+1),j))   \n",
    "    print(\"\\n\")\n",
    "    print(\"The highest PageRanks for the Personalized PageRank are:\")    \n",
    "    ## Adds one to the papers, because index 0 is equivalent to line 1 in the dataset/\n",
    "    for i,j in top_ppr:\n",
    "        print(\"Paper {}, with PageRank {}\".format(int(i+1),j))   \n",
    "\n",
    "gets_top_ranks(pr, ppr, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
